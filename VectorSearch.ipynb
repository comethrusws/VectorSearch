{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMHXe3pTn2kP40b3G7st+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comethrusws/VectorSearch/blob/main/VectorSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7vww9pj7Kctl"
      },
      "outputs": [],
      "source": [
        "!pip install langchain faiss-cpu transformers datasets pandas numpy sentence-transformers torch ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import HuggingFacePipeline, CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "LXhtCLOgKiR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_models():\n",
        "    \"\"\"\n",
        "    Set up the embedding model and LLM\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "    print(f\"Initialized embedding model: {embedding_model_name}\")\n",
        "\n",
        "\n",
        "    llm_local = CTransformers(\n",
        "        model=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
        "        model_file=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    # choose which LLM to use based on your hardware capabilities\n",
        "    # llm = llm_local  # For more powerful systems\n",
        "    llm = llm_pipeline  # For systems with less resources\n",
        "\n",
        "    return embeddings, llm"
      ],
      "metadata": {
        "id": "I9W-i3vrKkwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(directory_path):\n",
        "    \"\"\"\n",
        "    Load documents from a directory containing PDF and text files\n",
        "    \"\"\"\n",
        "    # Load PDFs\n",
        "    pdf_loader = DirectoryLoader(directory_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    pdf_documents = pdf_loader.load()\n",
        "\n",
        "    # Load text files\n",
        "    text_loader = DirectoryLoader(directory_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    text_documents = text_loader.load()\n",
        "\n",
        "    # Combine all documents\n",
        "    all_documents = pdf_documents + text_documents\n",
        "    print(f\"Loaded {len(all_documents)} documents\")\n",
        "    return all_documents\n"
      ],
      "metadata": {
        "id": "5OuJ3hQ5KvXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Split documents into chunks for better processing\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(chunks)} chunks\")\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "u8dpaxYlK0Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks, embeddings):\n",
        "    \"\"\"\n",
        "    Create embeddings for the document chunks and build a vector store\n",
        "    \"\"\"\n",
        "    # Create a FAISS vector store\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "    print(\"Vector store created successfully\")\n",
        "    return vector_store\n"
      ],
      "metadata": {
        "id": "kpaRmxM7K2u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(vector_store, query, k=5):\n",
        "    \"\"\"\n",
        "    Perform a basic semantic search\n",
        "    \"\"\"\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "Qt0OANXwK5_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_search(vector_store, query, metadata_filter=None, k=5):\n",
        "    \"\"\"\n",
        "    Perform advanced search with metadata filtering\n",
        "    \"\"\"\n",
        "    if metadata_filter:\n",
        "        docs = vector_store.similarity_search(\n",
        "            query,\n",
        "            k=k,\n",
        "            filter=metadata_filter\n",
        "        )\n",
        "    else:\n",
        "        docs = vector_store.similarity_search(query, k=k)\n",
        "\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "9IsaZSDkK8Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hybrid_retriever(vector_store, documents):\n",
        "    \"\"\"\n",
        "    Create a hybrid retriever combining vector search with BM25\n",
        "    \"\"\"\n",
        "    from langchain.retrievers import BM25Retriever\n",
        "    from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "    # Create BM25 retriever\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "    bm25_retriever.k = 10\n",
        "\n",
        "    # Create vector store retriever\n",
        "    vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "    # Create ensemble retriever\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[bm25_retriever, vector_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "    return ensemble_retriever"
      ],
      "metadata": {
        "id": "DWhKxE9-K_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rag_chain(retriever, llm):\n",
        "    \"\"\"\n",
        "    Create a RAG chain using the retriever and LLM\n",
        "    \"\"\"\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    return rag_chain"
      ],
      "metadata": {
        "id": "QxFgAhzqLBfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a sample dataset (ArXiv papers dataset)\n",
        "dataset = load_dataset(\"arxiv_dataset\", split=\"train[:100]\")\n",
        "print(f\"Loaded {len(dataset)} research paper abstracts\")\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=row[\"abstract\"],\n",
        "        metadata={\"title\": row[\"title\"], \"authors\": row[\"authors\"], \"categories\": row[\"categories\"]}\n",
        "    )\n",
        "    for row in dataset\n",
        "]\n",
        "\n",
        "\n",
        "embeddings, llm = setup_models()\n",
        "\n",
        "# Split documents into chunks\n",
        "chunks = split_documents(documents, chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "# Create vector store\n",
        "vector_store = create_vector_store(chunks, embeddings)\n",
        "\n",
        "# Create a standard retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "\n",
        "def query_rag(rag_chain, query):\n",
        "    \"\"\"\n",
        "    Query the RAG system\n",
        "    \"\"\"\n",
        "    result = rag_chain({\"query\": query})\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result['result']}\")\n",
        "    print(\"\\nSource Documents:\")\n",
        "\n",
        "    for i, doc in enumerate(result[\"source_documents\"]):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(f\"Content: {doc.page_content[:200]}...\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "LYky9FL4LFXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RAG chain\n",
        "rag_chain = create_rag_chain(retriever, llm)\n",
        "\n",
        "# Example query\n",
        "query = \"What are recent advances in quantum computing?\"\n",
        "result = query_rag(rag_chain, query)\n",
        "\n",
        "# Cell 14: Metadata Filtering\n",
        "# Example of search with metadata filtering\n",
        "query = \"neural networks in computer vision\"\n",
        "metadata_filter = {\"categories\": \"cs.CV\"}  # Filter for computer vision papers\n",
        "\n",
        "filtered_docs = advanced_search(vector_store, query, metadata_filter)\n",
        "\n",
        "print(f\"Query: {query} (filtered by {metadata_filter})\")\n",
        "for i, doc in enumerate(filtered_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Content: {doc.page_content[:200]}...\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "# Cell 15: Create Hybrid Retriever\n",
        "hybrid_retriever = create_hybrid_retriever(vector_store, documents)\n",
        "\n",
        "# Create RAG chain with hybrid retriever\n",
        "hybrid_rag_chain = create_rag_chain(hybrid_retriever, llm)\n",
        "\n",
        "# Compare results\n",
        "query = \"machine learning applications in healthcare\"\n",
        "print(\"Standard RAG Results:\")\n",
        "standard_result = query_rag(rag_chain, query)\n",
        "\n",
        "print(\"\\n\\nHybrid RAG Results:\")\n",
        "hybrid_result = query_rag(hybrid_rag_chain, query)"
      ],
      "metadata": {
        "id": "iBHvH69zLODr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the vector store to disk\n",
        "vector_store.save_local(\"my_faiss_index\")\n",
        "\n",
        "print(\"Vector store saved successfully\")\n",
        "\n",
        "# Cell 17: Load Vector Store for Future Use\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Load the vector store from disk\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "loaded_vector_store = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
        "\n",
        "print(\"Vector store loaded successfully\")\n",
        "\n",
        "# Cell 18: Advanced RAG Implementation with Source Context Management\n",
        "def enhanced_rag_query(query, vector_store, llm, max_tokens=4000):\n",
        "    \"\"\"\n",
        "    Enhanced RAG query that manages context window and properly formats sources\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    docs = vector_store.similarity_search(query, k=8)\n",
        "\n",
        "    # Format context with sources\n",
        "    context = \"\"\n",
        "    sources = []\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        # Format source reference\n",
        "        source_ref = f\"[{i+1}] {doc.metadata.get('title', f'Document {i+1}')}\"\n",
        "        sources.append(source_ref)\n",
        "\n",
        "        # Format document content with source reference\n",
        "        doc_content = f\"Source {i+1}: {doc.page_content}\"\n",
        "        context += doc_content + \"\\n\\n\"\n",
        "\n",
        "    # Create prompt\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question based on the provided context.\n",
        "    If you don't know the answer based on the context, just say so.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Please cite your sources using the numbers in square brackets (e.g., [1], [2]).\n",
        "    \"\"\"\n",
        "\n",
        "    # Get response from LLM\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import LLMChain\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"context\", \"query\"],\n",
        "        template=prompt\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    response = chain.run(context=context, query=query)\n",
        "\n",
        "    # Format the final answer with source references\n",
        "    final_answer = {\n",
        "        \"answer\": response,\n",
        "        \"sources\": sources\n",
        "    }\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "p28QaiZrLRO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_result = enhanced_rag_query(\n",
        "    \"What are the applications of reinforcement learning in robotics?\",\n",
        "    vector_store,\n",
        "    llm\n",
        ")\n",
        "\n",
        "print(f\"Answer: {enhanced_result['answer']}\\n\")\n",
        "print(\"Sources:\")\n",
        "for source in enhanced_result['sources']:\n",
        "    print(f\"- {source}\")\n",
        "\n",
        "# Cell 20: Evaluation of RAG System\n",
        "def evaluate_rag(vector_store, test_questions, ground_truth, llm):\n",
        "    \"\"\"\n",
        "    Evaluate the RAG system using test questions and ground truth\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for q, truth in zip(test_questions, ground_truth):\n",
        "        # Get RAG response\n",
        "        rag_response = enhanced_rag_query(q, vector_store, llm)\n",
        "\n",
        "        # Create evaluation prompt\n",
        "        eval_prompt = f\"\"\"\n",
        "        Question: {q}\n",
        "\n",
        "        Ground Truth Answer: {truth}\n",
        "\n",
        "        RAG System Answer: {rag_response['answer']}\n",
        "\n",
        "        On a scale of 1-10, rate how well the RAG system answer matches the ground truth in terms of:\n",
        "        1. Factual correctness (1-10)\n",
        "        2. Completeness (1-10)\n",
        "        3. Relevance (1-10)\n",
        "\n",
        "        Provide the three scores and a brief explanation for each.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get evaluation using a pipeline\n",
        "        from langchain.prompts import PromptTemplate\n",
        "        from langchain.chains import LLMChain\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"eval_prompt\"],\n",
        "            template=\"{eval_prompt}\"\n",
        "        )\n",
        "\n",
        "        chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "        evaluation = chain.run(eval_prompt=eval_prompt)\n",
        "\n",
        "        results.append({\n",
        "            \"question\": q,\n",
        "            \"ground_truth\": truth,\n",
        "            \"rag_answer\": rag_response['answer'],\n",
        "            \"evaluation\": evaluation\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Cell 21: Additional - Demonstrating MMR (Maximal Marginal Relevance) for Diversity\n",
        "def diverse_search(vector_store, query, k=5):\n",
        "    \"\"\"\n",
        "    Perform a diverse search using Maximal Marginal Relevance\n",
        "    \"\"\"\n",
        "    docs = vector_store.max_marginal_relevance_search(\n",
        "        query,\n",
        "        k=k,  # Number of documents to return\n",
        "        fetch_k=15,  # Fetch more documents, then rerank\n",
        "        lambda_mult=0.7  # Diversity factor (0 = max diversity, 1 = standard search)\n",
        "    )\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Example of MMR search\n",
        "query = \"deep learning applications\"\n",
        "diverse_docs = diverse_search(vector_store, query)\n",
        "\n",
        "print(f\"Diverse search results for: {query}\")\n",
        "for i, doc in enumerate(diverse_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Title: {doc.metadata.get('title', 'Unknown')}\")\n",
        "    print(f\"Content: {doc.page_content[:150]}...\")\n"
      ],
      "metadata": {
        "id": "HJmViI_tLWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compressed_retrieval(vector_store, query, k=5):\n",
        "    \"\"\"\n",
        "    Perform a search with contextual compression\n",
        "    \"\"\"\n",
        "    from langchain.retrievers import ContextualCompressionRetriever\n",
        "    from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "    # Create the base retriever\n",
        "    base_retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # Create the document compressor\n",
        "    compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "    # Create the contextual compression retriever\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    # Retrieve compressed documents\n",
        "    compressed_docs = compression_retriever.get_relevant_documents(query)\n",
        "    return compressed_docs\n",
        "\n",
        "# Example of compressed retrieval\n",
        "query = \"quantum computing algorithms\"\n",
        "compressed_docs = compressed_retrieval(vector_store, query)\n",
        "\n",
        "print(f\"Compressed retrieval results for: {query}\")\n",
        "for i, doc in enumerate(compressed_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Title: {doc.metadata.get('title', 'Unknown')}\")\n",
        "    print(f\"Content: {doc.page_content[:150]}...\")"
      ],
      "metadata": {
        "id": "eM_vezc0LZo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}